{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comoponents needed to build a RAG System\n",
    "\n",
    "- https://python.langchain.com/v0.2/docs/tutorials/retrievers/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install langchain-chroma langchain  langchain-openai\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "import warnings\n",
    "from IPython.display import display, Markdown  # to see better the output text\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "_ = load_dotenv(find_dotenv())  # read local .env file\n",
    "\n",
    "llm_model = \"gpt-3.5-turbo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "# If you want to get best in-class automated tracing of your model calls you\n",
    "# can also set your LangSmith API key\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass(\"Enter your LangSmith API key: \")\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documents\n",
    "https://python.langchain.com/v0.2/api_reference/core/documents/langchain_core.documents.base.Document.html\n",
    "\n",
    "LangChain implements a Document abstraction, which is intended to represent a unit of text and associated metadata.\n",
    "* `id`: an optional identifier for the document.\n",
    "* `page_content`: a string representing the content;\n",
    "* `metadata`: a dict containing arbitrary metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "documents = [\n",
    "    Document(id='1',\n",
    "             page_content=\"Dogs are great companions, known for their loyalty and friendliness.\",\n",
    "             metadata={\"source\": \"mammal-pets-doc\", \"user\": \"manuel\"},\n",
    "             ),\n",
    "    Document(id='2',\n",
    "             page_content=\"Cats are independent pets that often enjoy their own space.\",\n",
    "             metadata={\"source\": \"mammal-pets-doc\", \"user\": \"manuel\"},\n",
    "             ),\n",
    "    Document(id='3',\n",
    "             page_content=\"Goldfish are popular pets for beginners, requiring relatively simple care.\",\n",
    "             metadata={\"source\": \"fish-pets-doc\", \"user\": \"manuel\"},\n",
    "             ),\n",
    "    Document(id='4',\n",
    "             page_content=\"Parrots are intelligent birds capable of mimicking human speech.\",\n",
    "             metadata={\"source\": \"bird-pets-doc\", \"user\": \"alejandro\"},\n",
    "             ),\n",
    "    Document(id='5',\n",
    "             page_content=\"Rabbits are social animals that need plenty of space to hop around.\",\n",
    "             metadata={\"source\": \"mammal-pets-doc\", \"user\": \"alejandro\"},\n",
    "             ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Store\n",
    "LangChain includes a suite of integrations with different vector store technologies. Some vector stores are hosted by a provider (e.g., various cloud providers) and require specific credentials to use; some (such as Postgres) run in separate infrastructure that can be run locally or via a third-party; others can run in-memory for lightweight workloads. Here we will demonstrate usage of LangChain VectorStores using `Chroma`, which includes an in-memory implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ⚠️⚠️ **IMPORTANT:** vectorstores in langchain don't return the same document storaged; instead they return a copy of them and it is because that they don't return the `id` of the original document, so it will depend of the specific vectorstore implementation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chroma VectorStore\n",
    "\n",
    "> ⚠️⚠️ **IMPORTANT:** In langchain with `Chroma` vectorstore, the documents have ids but when we do the search their ids are not returned. The problem comes when we want to delete a document we only have the original id of the document but in the vectorstore this document is represented by many small documents that we do not know their ids.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# even if each document has its own id it is necessary to pass the list of ids \n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents,\n",
    "    ids=[doc.id for doc in documents],  # type: ignore\n",
    "    embedding=OpenAIEmbeddings(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'mammal-pets-doc', 'user': 'manuel'}, page_content='Dogs are great companions, known for their loyalty and friendliness.'),\n",
       " Document(metadata={'source': 'mammal-pets-doc', 'user': 'manuel'}, page_content='Cats are independent pets that often enjoy their own space.'),\n",
       " Document(metadata={'source': 'fish-pets-doc', 'user': 'manuel'}, page_content='Goldfish are popular pets for beginners, requiring relatively simple care.')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = vectorstore.similarity_search(\n",
    "    query=\"land animals\", k=3, filter={\"user\": \"manuel\"})\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(metadata={'source': 'bird-pets-doc', 'user': 'alejandro'}, page_content='Parrots are intelligent birds capable of mimicking human speech.'),\n",
       "  0.34512820839881897),\n",
       " (Document(metadata={'source': 'mammal-pets-doc', 'user': 'alejandro'}, page_content='Rabbits are social animals that need plenty of space to hop around.'),\n",
       "  0.45866116881370544),\n",
       " (Document(metadata={'source': 'mammal-pets-doc', 'user': 'manuel'}, page_content='Dogs are great companions, known for their loyalty and friendliness.'),\n",
       "  0.4592510163784027),\n",
       " (Document(metadata={'source': 'fish-pets-doc', 'user': 'manuel'}, page_content='Goldfish are popular pets for beginners, requiring relatively simple care.'),\n",
       "  0.47476130723953247)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note that providers implement different scores; Chroma here\n",
    "# returns a distance metric that should vary inversely with similarity.\n",
    "\n",
    "vectorstore.similarity_search_with_score(query=\"birds\",) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Async query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'bird-pets-doc', 'user': 'alejandro'}, page_content='Parrots are intelligent birds capable of mimicking human speech.'),\n",
       " Document(metadata={'source': 'mammal-pets-doc', 'user': 'alejandro'}, page_content='Rabbits are social animals that need plenty of space to hop around.'),\n",
       " Document(metadata={'source': 'mammal-pets-doc', 'user': 'manuel'}, page_content='Dogs are great companions, known for their loyalty and friendliness.'),\n",
       " Document(metadata={'source': 'fish-pets-doc', 'user': 'manuel'}, page_content='Goldfish are popular pets for beginners, requiring relatively simple care.')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await vectorstore.asimilarity_search(query=\"birds\",)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To see all the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': ['1', '2', '3', '4', '5'],\n",
       " 'embeddings': None,\n",
       " 'metadatas': [{'source': 'mammal-pets-doc', 'user': 'manuel'},\n",
       "  {'source': 'mammal-pets-doc', 'user': 'manuel'},\n",
       "  {'source': 'fish-pets-doc', 'user': 'manuel'},\n",
       "  {'source': 'bird-pets-doc', 'user': 'alejandro'},\n",
       "  {'source': 'mammal-pets-doc', 'user': 'alejandro'}],\n",
       " 'documents': ['Dogs are great companions, known for their loyalty and friendliness.',\n",
       "  'Cats are independent pets that often enjoy their own space.',\n",
       "  'Goldfish are popular pets for beginners, requiring relatively simple care.',\n",
       "  'Parrots are intelligent birds capable of mimicking human speech.',\n",
       "  'Rabbits are social animals that need plenty of space to hop around.'],\n",
       " 'uris': None,\n",
       " 'data': None,\n",
       " 'included': ['metadatas', 'documents']}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore._collection.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, scores closer to zero show better similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's go most deeply in `chroma` initialization\n",
    "\n",
    "\n",
    "https://python.langchain.com/v0.2/docs/integrations/vectorstores/chroma/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "# from langchain_huggingface import HuggingFaceEmbeddings\n",
    "# %pip install -qU langchain-huggingface\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "# using HuggingFace \n",
    "# other \"BAAI/bge-large-en-v1.5\"\n",
    "# embeddings = HuggingFaceEmbeddings(model=\"sentence-transformers/all-mpnet-base-v2\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"example_collection\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"./chroma_langchain_db\",  # Where to save data locally, remove if not neccesary\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization from client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.utils.embedding_functions import OpenAIEmbeddingFunction\n",
    "\n",
    "persistent_client = chromadb.PersistentClient(path=\"./chroma_langchain_db\")\n",
    "emb_funct = OpenAIEmbeddingFunction(\n",
    "    model_name=\"text-embedding-3-large\", api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "collection = persistent_client.get_or_create_collection(\n",
    "    \"collection_name\", embedding_function=emb_funct)\n",
    "collection.add(ids=[\"1\", \"2\", \"3\"], documents=[\"a\", \"b\", \"c\"])\n",
    "\n",
    "vector_store_from_client = Chroma(\n",
    "    client=persistent_client,\n",
    "    collection_name=\"collection_name\",\n",
    "    embedding_function=embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qdrant VectorStore\n",
    "\n",
    "https://python.langchain.com/v0.2/docs/integrations/vectorstores/qdrant/\n",
    "\n",
    "There are various modes of how to run `Qdrant`:\n",
    "* Local mode, no server required\n",
    "* Docker deployments\n",
    "* Qdrant Cloud\n",
    " \n",
    "  \n",
    "> ⚠️⚠️ **IMPORTANT:** In langchain with `Qdrant` vectorstore, the documents have ids but when we do the search their ids are returned inside od metadata in the `_id` field. The problem comes when we want to delete a document we only have the original id of the document but in the vectorstore this document is represented by many small documents that we do not know their ids.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "# %pip install -qU langchain-huggingface\n",
    "\n",
    "# using HuggingFace \n",
    "# other \"BAAI/bge-large-en-v1.5\", \"sentence-transformers/all-mpnet-base-v2\"\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-large-en-v1.5\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -qU langchain-qdrant\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams\n",
    "\n",
    "# if you may prefer to keep all the data in memory only,\n",
    "# use location='memory' instead of path='qdrant_db'\n",
    "client = QdrantClient(path='qdrant_db')\n",
    "\n",
    "client.create_collection(\n",
    "    collection_name=\"example_collection\",\n",
    "    vectors_config=VectorParams(size=1024, distance=Distance.COSINE),\n",
    ")\n",
    "\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=\"example_collection\",\n",
    "    embedding=embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['5f67c4a8ae364ec78a2bddb97f2f7fce',\n",
       " '4086d13af7e34d5285e58232a7fadd54',\n",
       " '5251c1a4532d40cba84a512ccd11d6b8',\n",
       " '33a60e6bfa104650b51745e8d3b18413',\n",
       " 'afb3882eec5a4a859e63f1dda45002b2']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from uuid import uuid4\n",
    "\n",
    "# uuids = [str(uuid4()) for _ in range(len(documents))]\n",
    "\n",
    "vector_store.add_documents(documents=documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(metadata={'source': 'bird-pets-doc', 'user': 'alejandro', '_id': '33a60e6bfa104650b51745e8d3b18413', '_collection_name': 'example_collection'}, page_content='Parrots are intelligent birds capable of mimicking human speech.'),\n",
       "  0.7719386919123767),\n",
       " (Document(metadata={'source': 'mammal-pets-doc', 'user': 'alejandro', '_id': 'afb3882eec5a4a859e63f1dda45002b2', '_collection_name': 'example_collection'}, page_content='Rabbits are social animals that need plenty of space to hop around.'),\n",
       "  0.5753312848911787)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vector_store.similarity_search_with_score( \"intelligent birds\", k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QdrantVectorStore supports 3 modes for similarity searches. They can be configured using the `retrieval_mode` parameter when setting up the class.\n",
    "\n",
    "* Dense Vector Search(Default) : To search with only dense vectors,\n",
    "* Sparse Vector Search : To search with only sparse vectors.\n",
    "* Hybrid Search: To perform a hybrid search using dense and sparse vectors with score fusion, `retrieval_mode = RetrievalMode.HYBRID`\n",
    "A dense `Embeddings` value should be provided to the embedding parameter and an implementation of the `SparseEmbeddings` interface using any sparse embeddings provider has to be provided as value to the sparse_embedding parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hybrid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 29 files: 100%|██████████| 29/29 [00:00<00:00, 312685.90it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(Document(metadata={'source': 'bird-pets-doc', 'user': 'alejandro', '_id': '14edb84710bd429581e78835642d19db', '_collection_name': 'my_documents'}, page_content='Parrots are intelligent birds capable of mimicking human speech.'),\n",
       "  1.0),\n",
       " (Document(metadata={'source': 'mammal-pets-doc', 'user': 'alejandro', '_id': '765cf028cb4c496cab64935fa8b8ebeb', '_collection_name': 'my_documents'}, page_content='Rabbits are social animals that need plenty of space to hop around.'),\n",
       "  0.3333333333333333)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %pip install fastembed\n",
    "from langchain_qdrant import FastEmbedSparse, RetrievalMode\n",
    "\n",
    "sparse_embeddings = FastEmbedSparse(model_name=\"Qdrant/bm25\")\n",
    "\n",
    "qdrant = QdrantVectorStore.from_documents(\n",
    "    documents,\n",
    "    embedding=embeddings,\n",
    "    sparse_embedding=sparse_embeddings,\n",
    "    location=\":memory:\",\n",
    "    collection_name=\"my_documents\",\n",
    "    retrieval_mode=RetrievalMode.HYBRID,\n",
    ")\n",
    "\n",
    "found_docs = qdrant.similarity_search_with_score(\"intelligent birds\", k=2)\n",
    "found_docs ## check like the first document has the score equal to 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata filtering\n",
    "https://qdrant.tech/documentation/concepts/filtering/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(metadata={'source': 'mammal-pets-doc', 'user': 'alejandro', '_id': 'afb3882eec5a4a859e63f1dda45002b2', '_collection_name': 'example_collection'}, page_content='Rabbits are social animals that need plenty of space to hop around.'),\n",
       "  0.614272780516637)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from qdrant_client.http.models import models\n",
    "\n",
    "# to see internal structure of the point of text in Qdrant\n",
    "# vector_store.client._client.search(\"example_collection\",embeddings.embed_query(\"animals\"))\n",
    "\n",
    "results = vector_store.similarity_search_with_score(\n",
    "    query=\"animals\",\n",
    "    k=4,\n",
    "    filter=models.Filter(\n",
    "        must=[\n",
    "            models.FieldCondition(\n",
    "                key=\"page_content\",\n",
    "                match=models.MatchValue(\n",
    "                    value=\"Rabbits are social animals that need plenty of space to hop around.\"\n",
    "                ),\n",
    "            ),\n",
    "            models.FieldCondition(\n",
    "                key=\"page_content\",\n",
    "                match=models.MatchText(\n",
    "                    text=\"are \"  # contain the word \"are\"\n",
    "                ),\n",
    "            ),\n",
    "           models.FieldCondition(\n",
    "                key=\"metadata.user\",\n",
    "                match=models.MatchText(text=\"alejandro\"),\n",
    "            ),\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrievers\n",
    "LangChain VectorStore objects do not subclass `Runnable`, and so cannot immediately be integrated into **LangChain Expression Language(LCEL)** chains.\n",
    "\n",
    "LangChain Retrievers are Runnables, so they implement a standard set of methods (e.g., synchronous and asynchronous invoke and batch operations) and are designed to be incorporated in LCEL chains.\n",
    "\n",
    "VectorStoreRetriever supports search types of `similarity` (default), `mmr` (maximum marginal relevance, described above), and `similarity_score_threshold`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'bird-pets-doc', 'user': 'alejandro'}, page_content='Parrots are intelligent birds capable of mimicking human speech.'),\n",
       " Document(metadata={'source': 'mammal-pets-doc', 'user': 'manuel'}, page_content='Dogs are great companions, known for their loyalty and friendliness.')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"mmr\", search_kwargs={\"k\": 2, \"fetch_k\": 5}\n",
    ")\n",
    "retriever.invoke(\"Intelligent animals\", filter={\"user\": \"manu\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "message = \"\"\"\n",
    "Answer this question using the provided context only.\n",
    "\n",
    "{question}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([(\"human\", message)])\n",
    "\n",
    "rag_chain = {\"context\": retriever, \"question\": RunnablePassthrough()} | prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The intelligent animals mentioned in the context are parrots.', response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 95, 'total_tokens': 106}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_507c9469a1', 'finish_reason': 'stop', 'logprobs': None}, id='run-6106b562-fea0-419d-b660-6614dacd7c25-0', usage_metadata={'input_tokens': 95, 'output_tokens': 11, 'total_tokens': 106})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"who are the Intelligent animals?\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Splitter/Chunking \n",
    "* https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb\n",
    "\n",
    "* https://python.langchain.com/v0.2/docs/how_to/#text-splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "One of the most important things I didn't understand about the world when I was a child is the degree to which the returns for performance are superlinear.\n",
    "\n",
    "Teachers and coaches implicitly told us the returns were linear. \"You get out,\" I heard a thousand times, \"what you put in.\" They meant well, but this is rarely true. If your product is only half as good as your competitor's, you don't get half as many customers. You get no customers, and you go out of business.\n",
    "\n",
    "It's obviously true that the returns for performance are superlinear in business. Some think this is a flaw of capitalism, and that if we changed the rules it would stop being true. But superlinear returns for performance are a feature of the world, not an artifact of rules we've invented. We see the same pattern in fame, power, military victories, knowledge, and even benefit to humanity. In all of these, the rich get richer. [1]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2= \"\"\"Formally, a \"database\" refers to a set of related data accessed through the use of a \"database management system\" (DBMS), which is an integrated set of computer software that allows users to interact with one or more databases and provides access to all of the data contained in the database (although restrictions may exist that limit access to particular data). The DBMS provides various functions that allow entry, storage and retrieval of large quantities of information and provides ways to manage how that information is organized. High-performance computing is critical for the processing and analysis of data. One particularly widespread approach to computing for data engineering is dataflow programming, in which the computation is represented as a directed graph (dataflow graph); nodes are the operations, and edges represent the flow of data. Popular implementations include Apache Spark, and the deep learning specific TensorFlow.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **RecursiveCharacterTextSplitter**\n",
    "It's the recommended one for generic text. It is parameterized by a list of characters. It tries to split on them in order until the chunks are small enough. The default list is `[\"\\n\\n\", \"\\n\", \" \", \"\"]`. This has the effect of trying to keep all paragraphs (and then sentences, and then words) together as long as possible, as those would generically seem to be the strongest semantically related pieces of text.\n",
    "\n",
    "How the text is split: \n",
    "1. by list of characters.\n",
    "2. How the chunk size is measured: by number of characters.\n",
    "\n",
    "To obtain the string content directly, use `.split_text`\n",
    "\n",
    "To create LangChain Document objects (e.g., for use in downstream tasks), use `.create_documents`\n",
    "\n",
    "> It can be difficult to find the best `chunk_size, chunk_overlap` settings to capture the actual relationships in the document.\n",
    "\n",
    "Parameters:\n",
    "* `chunk_size`: The maximum size of a chunk, where size is determined by the length_function.\n",
    "* `chunk_overlap`: Target overlap between chunks. Overlapping chunks helps to mitigate loss of information when context is divided between chunks.\n",
    "* `length_function`: Function determining the chunk size.\n",
    "* `is_separator_regex`: Whether the separator list (defaulting to [\"\\n\\n\", \"\\n\", \" \", \"\"]) should be interpreted as regex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['One of the most important things I didn\\'t understand about the world when I was a child is the degree to which the returns for performance are superlinear.\\n\\nTeachers and coaches implicitly told us the returns were linear. \\n\\n\"You get out,\" I heard a thousand times, \"what you put in.\" They meant well, but this is rarely true.',\n",
       " 'Teachers and coaches implicitly told us the returns were linear. \\n\\n\"You get out,\" I heard a thousand times, \"what you put in.\" They meant well, but this is rarely true.\\n\\nIf your product is only half as good as your competitor\\'s, you don\\'t get half as many customers. You get no customers, and you go out of business.',\n",
       " \"It's obviously true that the returns for performance are superlinear in business. Some think this is a flaw of capitalism, and that if we changed the rules it would stop being true. But superlinear returns for performance are a feature of the world, not an artifact of rules we've invented\",\n",
       " \". But superlinear returns for performance are a feature of the world, not an artifact of rules we've invented. We see the same pattern in fame, power, military victories, knowledge, and even benefit to humanity. In all of these, the rich get richer. [1]\"]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 350, chunk_overlap=200,\n",
    "separators=[\"\\n\\n\", \"\\n\",\".\"])\n",
    "\n",
    "text_splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Split text based on semantic similarity**\n",
    "\n",
    "https://python.langchain.com/v0.2/docs/how_to/semantic-chunker/\n",
    "\n",
    "\n",
    "At a high level, this splits into sentences, then groups into groups of 3 sentences, and then merges one that are similar in the embedding space. If embeddings are sufficiently far apart, chunks are split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Name: langchain-experimental\n",
      "Version: 0.0.64\n",
      "Summary: Building applications with LLMs through composability\n",
      "Home-page: https://github.com/langchain-ai/langchain\n",
      "Author: \n",
      "Author-email: \n",
      "License: MIT\n",
      "Location: f:\\documentos\\data_science\\large language models llm\\langchain_library_08_24\\.venv\\lib\\site-packages\n",
      "Requires: langchain-community, langchain-core\n",
      "Required-by: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution - (f:\\documentos\\data_science\\large language models llm\\langchain_library_08_24\\.venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (f:\\documentos\\data_science\\large language models llm\\langchain_library_08_24\\.venv\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "%pip show langchain_experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "# %pip install --quiet langchain_experimental\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "semantic_splitter = SemanticChunker(OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='\\nOne of the most important things I didn\\'t understand about the world when I was a child is the degree to which the returns for performance are superlinear. Teachers and coaches implicitly told us the returns were linear. \"You get out,\" I heard a thousand times, \"what you put in.\" They meant well, but this is rarely true. If your product is only half as good as your competitor\\'s, you don\\'t get half as many customers.'),\n",
       " Document(page_content=\"You get no customers, and you go out of business. It's obviously true that the returns for performance are superlinear in business. Some think this is a flaw of capitalism, and that if we changed the rules it would stop being true. But superlinear returns for performance are a feature of the world, not an artifact of rules we've invented. We see the same pattern in fame, power, military victories, knowledge, and even benefit to humanity. In all of these, the rich get richer. [1]\\n\")]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semantic_out = semantic_splitter.create_documents([text])\n",
    "semantic_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formally, a \"database\" refers to a set of related data accessed through the use of a \"database management system\" (DBMS), which is an integrated set of computer software that allows users to interact with one or more databases and provides access to all of the data contained in the database (although restrictions may exist that limit access to particular data). The DBMS provides various functions that allow entry, storage and retrieval of large quantities of information and provides ways to manage how that information is organized.\n",
      "High-performance computing is critical for the processing and analysis of data. One particularly widespread approach to computing for data engineering is dataflow programming, in which the computation is represented as a directed graph (dataflow graph); nodes are the operations, and edges represent the flow of data. Popular implementations include Apache Spark, and the deep learning specific TensorFlow.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = semantic_splitter.split_text(text2)\n",
    "[print(_) for _ in out]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chunker works by determining when to \"break\" apart sentences. This is done by looking for differences in embeddings between any two sentences. When that difference is past **some threshold**, then they are split.\n",
    "\n",
    "There are a few ways to determine what that threshold is, which are controlled by the `breakpoint_threshold_type` kwarg:\n",
    "\n",
    "- `percentile`: the default way to split is based on percentile. In this method, all differences between sentences are calculated, and then any difference greater than the `breakpoint_threshold_amount` percentile is split. In this type the `breakpoint_threshold_amount=95` by default, it is like a confidence interval. So,  if you want more chunks, lower the percentile cutoff.\n",
    "\n",
    "- `standard_deviation`: in this method, any difference greater than `breakpoint_threshold_amount` standard deviations is split. In this type the `breakpoint_threshold_amount=3` by default. \n",
    "\n",
    "- `interquartile` In this method, the interquartile distance is used to split chunks.In this type the `breakpoint_threshold_amount=1.5` by default. \n",
    "\n",
    "- `gradient`: in this method, the gradient of distance is used to split chunks along with the percentile method. This method is useful when chunks are highly correlated with each other or specific to a domain e.g. legal or medical. The idea is to apply anomaly detection on gradient array so that the distribution become wider and easy to identify boundaries in highly semantic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nOne of the most important things I didn\\'t understand about the world when I was a child is the degree to which the returns for performance are superlinear. Teachers and coaches implicitly told us the returns were linear. \"You get out,\" I heard a thousand times, \"what you put in.\" They meant well, but this is rarely true.',\n",
       " \"If your product is only half as good as your competitor's, you don't get half as many customers.\",\n",
       " \"You get no customers, and you go out of business. It's obviously true that the returns for performance are superlinear in business. Some think this is a flaw of capitalism, and that if we changed the rules it would stop being true. But superlinear returns for performance are a feature of the world, not an artifact of rules we've invented. We see the same pattern in fame, power, military victories, knowledge, and even benefit to humanity.\",\n",
       " 'In all of these, the rich get richer.',\n",
       " '[1]\\n']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss = SemanticChunker(OpenAIEmbeddings(\n",
    "), breakpoint_threshold_type='percentile', breakpoint_threshold_amount=60)\n",
    "ss.split_text(text) # we can see more chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">A good strategy is split the text into large chunks with **SemanticChunker** and then split them into smaller chunks with **RecursiveCharacterTextSplitter** to ensure that the document has maximum size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='One of the most important things I didn\\'t understand about the world when I was a child is the degree to which the returns for performance are superlinear. Teachers and coaches implicitly told us the returns were linear. \"You get out,\" I heard a thousand times, \"what you put in.\" They meant well, but this is rarely true'),\n",
       " Document(page_content='. Teachers and coaches implicitly told us the returns were linear. \"You get out,\" I heard a thousand times, \"what you put in.\" They meant well, but this is rarely true. If your product is only half as good as your competitor\\'s, you don\\'t get half as many customers.'),\n",
       " Document(page_content=\"You get no customers, and you go out of business. It's obviously true that the returns for performance are superlinear in business. Some think this is a flaw of capitalism, and that if we changed the rules it would stop being true. But superlinear returns for performance are a feature of the world, not an artifact of rules we've invented\"),\n",
       " Document(page_content=\". But superlinear returns for performance are a feature of the world, not an artifact of rules we've invented. We see the same pattern in fame, power, military victories, knowledge, and even benefit to humanity. In all of these, the rich get richer. [1]\")]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter.split_documents(semantic_out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
