{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comoponents needed to build a RAG System\n",
    "\n",
    "- https://python.langchain.com/v0.2/docs/tutorials/retrievers/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install langchain-chroma langchain  langchain-openai\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "import warnings\n",
    "from IPython.display import display, Markdown  # to see better the output text\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "_ = load_dotenv(find_dotenv())  # read local .env file\n",
    "\n",
    "llm_model = \"gpt-3.5-turbo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "# If you want to get best in-class automated tracing of your model calls you\n",
    "# can also set your LangSmith API key\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass(\"Enter your LangSmith API key: \")\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documents\n",
    "https://python.langchain.com/v0.2/api_reference/core/documents/langchain_core.documents.base.Document.html\n",
    "\n",
    "LangChain implements a Document abstraction, which is intended to represent a unit of text and associated metadata.\n",
    "* `id`: an optional identifier for the document.\n",
    "* `page_content`: a string representing the content;\n",
    "* `metadata`: a dict containing arbitrary metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "documents = [\n",
    "    Document(id='1',\n",
    "             page_content=\"Dogs are great companions, known for their loyalty and friendliness.\",\n",
    "             metadata={\"source\": \"mammal-pets-doc\", \"user\": \"manuel\"},\n",
    "             ),\n",
    "    Document(id='2',\n",
    "             page_content=\"Cats are independent pets that often enjoy their own space.\",\n",
    "             metadata={\"source\": \"mammal-pets-doc\", \"user\": \"manuel\"},\n",
    "             ),\n",
    "    Document(id='3',\n",
    "             page_content=\"Goldfish are popular pets for beginners, requiring relatively simple care.\",\n",
    "             metadata={\"source\": \"fish-pets-doc\", \"user\": \"manuel\"},\n",
    "             ),\n",
    "    Document(id='4',\n",
    "             page_content=\"Parrots are intelligent birds capable of mimicking human speech.\",\n",
    "             metadata={\"source\": \"bird-pets-doc\", \"user\": \"alejandro\"},\n",
    "             ),\n",
    "    Document(id='5',\n",
    "             page_content=\"Rabbits are social animals that need plenty of space to hop around.\",\n",
    "             metadata={\"source\": \"mammal-pets-doc\", \"user\": \"alejandro\"},\n",
    "             ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Store\n",
    "LangChain includes a suite of integrations with different vector store technologies. Some vector stores are hosted by a provider (e.g., various cloud providers) and require specific credentials to use; some (such as Postgres) run in separate infrastructure that can be run locally or via a third-party; others can run in-memory for lightweight workloads. Here we will demonstrate usage of LangChain VectorStores using `Chroma`, which includes an in-memory implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# even if each document has its own id it is necessary to pass the list of ids \n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents,\n",
    "    ids=[doc.id for doc in documents],  # type: ignore\n",
    "    embedding=OpenAIEmbeddings(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'mammal-pets-doc', 'user': 'manuel'}, page_content='Dogs are great companions, known for their loyalty and friendliness.'),\n",
       " Document(metadata={'source': 'mammal-pets-doc', 'user': 'manuel'}, page_content='Cats are independent pets that often enjoy their own space.'),\n",
       " Document(metadata={'source': 'fish-pets-doc', 'user': 'manuel'}, page_content='Goldfish are popular pets for beginners, requiring relatively simple care.')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = vectorstore.similarity_search(\n",
    "    query=\"land animals\", k=3, filter={\"user\": \"manuel\"})\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ⚠️⚠️ **IMPORTANT:** vectorstores in langchain don't return the same document storaged; instead they return a copy of them and it is because that they don't return the `id` of the original document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(metadata={'source': 'bird-pets-doc', 'user': 'alejandro'}, page_content='Parrots are intelligent birds capable of mimicking human speech.'),\n",
       "  0.34512820839881897),\n",
       " (Document(metadata={'source': 'mammal-pets-doc', 'user': 'alejandro'}, page_content='Rabbits are social animals that need plenty of space to hop around.'),\n",
       "  0.45866116881370544),\n",
       " (Document(metadata={'source': 'mammal-pets-doc', 'user': 'manuel'}, page_content='Dogs are great companions, known for their loyalty and friendliness.'),\n",
       "  0.4592510163784027),\n",
       " (Document(metadata={'source': 'fish-pets-doc', 'user': 'manuel'}, page_content='Goldfish are popular pets for beginners, requiring relatively simple care.'),\n",
       "  0.47476130723953247)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note that providers implement different scores; Chroma here\n",
    "# returns a distance metric that should vary inversely with similarity.\n",
    "\n",
    "vectorstore.similarity_search_with_score(query=\"birds\",) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Async query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'bird-pets-doc', 'user': 'alejandro'}, page_content='Parrots are intelligent birds capable of mimicking human speech.'),\n",
       " Document(metadata={'source': 'mammal-pets-doc', 'user': 'alejandro'}, page_content='Rabbits are social animals that need plenty of space to hop around.'),\n",
       " Document(metadata={'source': 'mammal-pets-doc', 'user': 'manuel'}, page_content='Dogs are great companions, known for their loyalty and friendliness.'),\n",
       " Document(metadata={'source': 'fish-pets-doc', 'user': 'manuel'}, page_content='Goldfish are popular pets for beginners, requiring relatively simple care.')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await vectorstore.asimilarity_search(query=\"birds\",)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To see all the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': ['1', '2', '3', '4', '5'],\n",
       " 'embeddings': None,\n",
       " 'metadatas': [{'source': 'mammal-pets-doc', 'user': 'manuel'},\n",
       "  {'source': 'mammal-pets-doc', 'user': 'manuel'},\n",
       "  {'source': 'fish-pets-doc', 'user': 'manuel'},\n",
       "  {'source': 'bird-pets-doc', 'user': 'alejandro'},\n",
       "  {'source': 'mammal-pets-doc', 'user': 'alejandro'}],\n",
       " 'documents': ['Dogs are great companions, known for their loyalty and friendliness.',\n",
       "  'Cats are independent pets that often enjoy their own space.',\n",
       "  'Goldfish are popular pets for beginners, requiring relatively simple care.',\n",
       "  'Parrots are intelligent birds capable of mimicking human speech.',\n",
       "  'Rabbits are social animals that need plenty of space to hop around.'],\n",
       " 'uris': None,\n",
       " 'data': None,\n",
       " 'included': ['metadatas', 'documents']}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore._collection.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, scores closer to zero show better similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's go most deeply in `chroma` initialization\n",
    "\n",
    "\n",
    "https://python.langchain.com/v0.2/docs/integrations/vectorstores/chroma/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "# from langchain_huggingface import HuggingFaceEmbeddings\n",
    "# %pip install -qU langchain-huggingface\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "# using HuggingFace \n",
    "# other \"BAAI/bge-large-en-v1.5\"\n",
    "# embeddings = HuggingFaceEmbeddings(model=\"sentence-transformers/all-mpnet-base-v2\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"example_collection\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"./chroma_langchain_db\",  # Where to save data locally, remove if not neccesary\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization from client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.utils.embedding_functions import OpenAIEmbeddingFunction\n",
    "\n",
    "persistent_client = chromadb.PersistentClient(path=\"./chroma_langchain_db\")\n",
    "emb_funct = OpenAIEmbeddingFunction(\n",
    "    model_name=\"text-embedding-3-large\", api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "collection = persistent_client.get_or_create_collection(\n",
    "    \"collection_name\", embedding_function=emb_funct)\n",
    "collection.add(ids=[\"1\", \"2\", \"3\"], documents=[\"a\", \"b\", \"c\"])\n",
    "\n",
    "vector_store_from_client = Chroma(\n",
    "    client=persistent_client,\n",
    "    collection_name=\"collection_name\",\n",
    "    embedding_function=embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrievers\n",
    "LangChain VectorStore objects do not subclass `Runnable`, and so cannot immediately be integrated into **LangChain Expression Language(LCEL)** chains.\n",
    "\n",
    "LangChain Retrievers are Runnables, so they implement a standard set of methods (e.g., synchronous and asynchronous invoke and batch operations) and are designed to be incorporated in LCEL chains.\n",
    "\n",
    "VectorStoreRetriever supports search types of `similarity` (default), `mmr` (maximum marginal relevance, described above), and `similarity_score_threshold`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'bird-pets-doc', 'user': 'alejandro'}, page_content='Parrots are intelligent birds capable of mimicking human speech.'),\n",
       " Document(metadata={'source': 'mammal-pets-doc', 'user': 'manuel'}, page_content='Dogs are great companions, known for their loyalty and friendliness.')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"mmr\", search_kwargs={\"k\": 2, \"fetch_k\": 5}\n",
    ")\n",
    "retriever.invoke(\"Intelligent animals\", filter={\"user\": \"manu\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "message = \"\"\"\n",
    "Answer this question using the provided context only.\n",
    "\n",
    "{question}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([(\"human\", message)])\n",
    "\n",
    "rag_chain = {\"context\": retriever, \"question\": RunnablePassthrough()} | prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The intelligent animals mentioned in the context are parrots.', response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 95, 'total_tokens': 106}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_507c9469a1', 'finish_reason': 'stop', 'logprobs': None}, id='run-6106b562-fea0-419d-b660-6614dacd7c25-0', usage_metadata={'input_tokens': 95, 'output_tokens': 11, 'total_tokens': 106})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"who are the Intelligent animals?\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Splitter/Chunking \n",
    "* https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb\n",
    "\n",
    "* https://python.langchain.com/v0.2/docs/how_to/#text-splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "One of the most important things I didn't understand about the world when I was a child is the degree to which the returns for performance are superlinear.\n",
    "\n",
    "Teachers and coaches implicitly told us the returns were linear. \"You get out,\" I heard a thousand times, \"what you put in.\" They meant well, but this is rarely true. If your product is only half as good as your competitor's, you don't get half as many customers. You get no customers, and you go out of business.\n",
    "\n",
    "It's obviously true that the returns for performance are superlinear in business. Some think this is a flaw of capitalism, and that if we changed the rules it would stop being true. But superlinear returns for performance are a feature of the world, not an artifact of rules we've invented. We see the same pattern in fame, power, military victories, knowledge, and even benefit to humanity. In all of these, the rich get richer. [1]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **RecursiveCharacterTextSplitter**\n",
    "It's the recommended one for generic text. It is parameterized by a list of characters. It tries to split on them in order until the chunks are small enough. The default list is `[\"\\n\\n\", \"\\n\", \" \", \"\"]`. This has the effect of trying to keep all paragraphs (and then sentences, and then words) together as long as possible, as those would generically seem to be the strongest semantically related pieces of text.\n",
    "\n",
    "How the text is split: \n",
    "1. by list of characters.\n",
    "2. How the chunk size is measured: by number of characters.\n",
    "\n",
    "To obtain the string content directly, use `.split_text`\n",
    "\n",
    "To create LangChain Document objects (e.g., for use in downstream tasks), use `.create_documents`\n",
    "\n",
    "> It can be difficult to find the best `chunk_size, chunk_overlap` settings to capture the actual relationships in the document.\n",
    "\n",
    "Parameters:\n",
    "* `chunk_size`: The maximum size of a chunk, where size is determined by the length_function.\n",
    "* `chunk_overlap`: Target overlap between chunks. Overlapping chunks helps to mitigate loss of information when context is divided between chunks.\n",
    "* `length_function`: Function determining the chunk size.\n",
    "* `is_separator_regex`: Whether the separator list (defaulting to [\"\\n\\n\", \"\\n\", \" \", \"\"]) should be interpreted as regex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['One of the most important things I didn\\'t understand about the world when I was a child is the degree to which the returns for performance are superlinear.\\n\\nTeachers and coaches implicitly told us the returns were linear. \\n\\n\"You get out,\" I heard a thousand times, \"what you put in.\" They meant well, but this is rarely true.',\n",
       " 'Teachers and coaches implicitly told us the returns were linear. \\n\\n\"You get out,\" I heard a thousand times, \"what you put in.\" They meant well, but this is rarely true.\\n\\nIf your product is only half as good as your competitor\\'s, you don\\'t get half as many customers. You get no customers, and you go out of business.',\n",
       " \"It's obviously true that the returns for performance are superlinear in business. Some think this is a flaw of capitalism, and that if we changed the rules it would stop being true. But superlinear returns for performance are a feature of the world, not an artifact of rules we've invented\",\n",
       " \". But superlinear returns for performance are a feature of the world, not an artifact of rules we've invented. We see the same pattern in fame, power, military victories, knowledge, and even benefit to humanity. In all of these, the rich get richer. [1]\"]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 350, chunk_overlap=200,\n",
    "separators=[\"\\n\\n\", \"\\n\",\".\"])\n",
    "\n",
    "text_splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Split text based on semantic similarity**\n",
    "\n",
    "https://python.langchain.com/v0.2/docs/how_to/semantic-chunker/\n",
    "\n",
    "\n",
    "At a high level, this splits into sentences, then groups into groups of 3 sentences, and then merges one that are similar in the embedding space. If embeddings are sufficiently far apart, chunks are split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "# %pip install --quiet langchain_experimental\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "semantic_splitter = SemanticChunker(OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "One of the most important things I didn't understand about the world when I was a child is the degree to which the returns for performance are superlinear. Teachers and coaches implicitly told us the returns were linear. \"You get out,\" I heard a thousand times, \"what you put in.\" They meant well, but this is rarely true. If your product is only half as good as your competitor's, you don't get half as many customers.\n",
      "You get no customers, and you go out of business. It's obviously true that the returns for performance are superlinear in business. Some think this is a flaw of capitalism, and that if we changed the rules it would stop being true. But superlinear returns for performance are a feature of the world, not an artifact of rules we've invented. We see the same pattern in fame, power, military victories, knowledge, and even benefit to humanity. In all of these, the rich get richer. [1]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = semantic_splitter.split_text(text)\n",
    "[print(_) for _ in out]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='\\nOne of the most important things I didn\\'t understand about the world when I was a child is the degree to which the returns for performance are superlinear. Teachers and coaches implicitly told us the returns were linear. \"You get out,\" I heard a thousand times, \"what you put in.\" They meant well, but this is rarely true. If your product is only half as good as your competitor\\'s, you don\\'t get half as many customers.'),\n",
       " Document(page_content=\"You get no customers, and you go out of business. It's obviously true that the returns for performance are superlinear in business. Some think this is a flaw of capitalism, and that if we changed the rules it would stop being true. But superlinear returns for performance are a feature of the world, not an artifact of rules we've invented. We see the same pattern in fame, power, military victories, knowledge, and even benefit to humanity. In all of these, the rich get richer. [1]\\n\")]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semantic_out = semantic_splitter.create_documents([text])\n",
    "semantic_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chunker works by determining when to \"break\" apart sentences. This is done by looking for differences in embeddings between any two sentences. When that difference is past **some threshold**, then they are split.\n",
    "\n",
    "There are a few ways to determine what that threshold is, which are controlled by the `breakpoint_threshold_type` kwarg:\n",
    "\n",
    "- `percentile`: the default way to split is based on percentile. In this method, all differences between sentences are calculated, and then any difference greater than the `breakpoint_threshold_amount` percentile is split. In this type the `breakpoint_threshold_amount=95` by default, it is like a confidence interval. So,  if you want more chunks, lower the percentile cutoff.\n",
    "\n",
    "- `standard_deviation`: in this method, any difference greater than `breakpoint_threshold_amount` standard deviations is split. In this type the `breakpoint_threshold_amount=3` by default. \n",
    "\n",
    "- `interquartile` In this method, the interquartile distance is used to split chunks.In this type the `breakpoint_threshold_amount=1.5` by default. \n",
    "\n",
    "- `gradient`: in this method, the gradient of distance is used to split chunks along with the percentile method. This method is useful when chunks are highly correlated with each other or specific to a domain e.g. legal or medical. The idea is to apply anomaly detection on gradient array so that the distribution become wider and easy to identify boundaries in highly semantic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nOne of the most important things I didn\\'t understand about the world when I was a child is the degree to which the returns for performance are superlinear. Teachers and coaches implicitly told us the returns were linear. \"You get out,\" I heard a thousand times, \"what you put in.\" They meant well, but this is rarely true.',\n",
       " \"If your product is only half as good as your competitor's, you don't get half as many customers.\",\n",
       " \"You get no customers, and you go out of business. It's obviously true that the returns for performance are superlinear in business. Some think this is a flaw of capitalism, and that if we changed the rules it would stop being true. But superlinear returns for performance are a feature of the world, not an artifact of rules we've invented. We see the same pattern in fame, power, military victories, knowledge, and even benefit to humanity.\",\n",
       " 'In all of these, the rich get richer.',\n",
       " '[1]\\n']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss = SemanticChunker(OpenAIEmbeddings(\n",
    "), breakpoint_threshold_type='percentile', breakpoint_threshold_amount=60)\n",
    "ss.split_text(text) # we can see more chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">A good strategy is split the text into large chunks with **SemanticChunker** and then split them into smaller chunks with **RecursiveCharacterTextSplitter** to ensure that the document has maximum size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='One of the most important things I didn\\'t understand about the world when I was a child is the degree to which the returns for performance are superlinear. Teachers and coaches implicitly told us the returns were linear. \"You get out,\" I heard a thousand times, \"what you put in.\" They meant well, but this is rarely true'),\n",
       " Document(page_content='. Teachers and coaches implicitly told us the returns were linear. \"You get out,\" I heard a thousand times, \"what you put in.\" They meant well, but this is rarely true. If your product is only half as good as your competitor\\'s, you don\\'t get half as many customers.'),\n",
       " Document(page_content=\"You get no customers, and you go out of business. It's obviously true that the returns for performance are superlinear in business. Some think this is a flaw of capitalism, and that if we changed the rules it would stop being true. But superlinear returns for performance are a feature of the world, not an artifact of rules we've invented\"),\n",
       " Document(page_content=\". But superlinear returns for performance are a feature of the world, not an artifact of rules we've invented. We see the same pattern in fame, power, military victories, knowledge, and even benefit to humanity. In all of these, the rich get richer. [1]\")]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter.split_documents(semantic_out)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
