{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Own RAG system\n",
    "\n",
    "* We will use as document to index the text in this post: https://lilianweng.github.io/posts/2023-06-23-agent/\n",
    "\n",
    "* All configurable variables must be extracted for the configuration \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cofiguration Variables\n",
    "\n",
    "from typing import Literal\n",
    "\n",
    "LLM_NAME = \"gpt-3.5-turbo\"\n",
    "EMBEDDING_NAME = \"BAAI/bge-large-en-v1.5\"\n",
    "\n",
    "CHUNK_SIZE = 1000\n",
    "CHUNK_OVERLAP = 250\n",
    "\n",
    "SEMANTIC_BREAKPOINT_TYPE: Literal[\n",
    "    \"percentile\", \"standard_deviation\", \"interquartile\", \"gradient\"\n",
    "] = \"percentile\"\n",
    "\n",
    "SEMANTIC_BREAKPOINT_VALUE = 95 # the default value, lower make more splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "import warnings\n",
    "from IPython.display import display, Markdown  # to see better the output text\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "_ = load_dotenv(find_dotenv())  # read local .env file\n",
    "\n",
    "# os.environ[\"LANGSMITH_API_KEY\"] = getting from .env file\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of documents loaded: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "43131"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import bs4 #beautifulsoup4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# Only keep post title, headers, and content from the full HTML.\n",
    "bs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\n",
    "\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs={\"parse_only\": bs4_strainer},\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "print(f\"Amount of documents loaded: {len(docs)}\")\n",
    "len(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create self-document with the text extracted from the HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "doc = Document(docs[0].page_content, metadata={\n",
    "               \"id\": \"fake_id\", \"user\": \"manuel\", \"collection\": \"blog\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We first use `SemanticChunker` to split the text into chunks and then use `RecursiveCharacterTextSplitter` to control the document size (we get started with `chunk_size=1000` and `chunk_overlap=250`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %pip install --quiet langchain_experimental\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_NAME)\n",
    "\n",
    "semantic_splitter = SemanticChunker(\n",
    "    embeddings,\n",
    "    breakpoint_threshold_type=SEMANTIC_BREAKPOINT_TYPE, breakpoint_threshold_amount=SEMANTIC_BREAKPOINT_VALUE)\n",
    "\n",
    "docs_split = semantic_splitter.split_documents([doc]) \n",
    "# semantic_splitter takes 110 seconds to run on a 43131 character document\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\"])\n",
    "\n",
    "all_splits = text_splitter.split_documents(docs_split)\n",
    "\n",
    "len(all_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "LSH (Locality-Sensitive Hashing): It introduces a hashing function such that similar input items are mapped to the same buckets with high probability, where the number of buckets is much smaller than the number of inputs. ANNOY (Approximate Nearest Neighbors Oh Yeah): The core data structure are random projection trees, a set of binary trees where each non-leaf node represents a hyperplane splitting the input space into half and each leaf stores one data point. Trees are built independently and at random, so to some extent, it mimics a hashing function. ANNOY search happens in all the trees to iteratively search through the half that is closest to the query and then aggregates the results. The idea is quite related to KD tree but a lot more scalable. HNSW (Hierarchical Navigable Small World): It is inspired by the idea of small world networks where most nodes can be reached by any other nodes within a small number of steps; e.g. “six degrees of separation” feature of social networks"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(all_splits[20].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We will use Qdrant like vectorstore and its Hybrid Search to retrieve documents.\n",
    "\n",
    "* We store the original `ID` of each document as a field in the metadata. To delete a document from the Qdrant vector store, we filter by this metadata field to retrieve a list of document IDs associated(using the `_id` metadata field). With this list of IDs, we can proceed to delete the documents that correspond to the original entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 29 files: 100%|██████████| 29/29 [00:00<00:00, 325226.78it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# %pip install -qU langchain-qdrant\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from langchain_qdrant import FastEmbedSparse, RetrievalMode\n",
    "# * \n",
    "\n",
    "qdrant_vectorstore = QdrantVectorStore.from_documents(\n",
    "    [],\n",
    "    embedding=embeddings,\n",
    "    sparse_embedding=FastEmbedSparse(model_name=\"Qdrant/bm25\"),\n",
    "    path='qdrant_db_hybrid1',\n",
    "    collection_name=\"my_documents\",\n",
    "    retrieval_mode=RetrievalMode.HYBRID,\n",
    ")\n",
    "\n",
    "# TODO: see the comportament of the FastEmbedSparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['19d0a46422c44e3f948e8f4268cedac0',\n",
       " 'd01ba413fb7843f6b8fcee8181fa7c6d',\n",
       " 'd4f94e3168cb4558b1e983b93252da18',\n",
       " '38393a10f106471a88abdedb2ffe5bc7',\n",
       " '29b49a2ae87c4bdb87750d1d45578e75',\n",
       " '38bc2ca2876947f19a7c17a8fdfb33df',\n",
       " '4beb0a0fee09445dafdf1324aa0facaa',\n",
       " 'c421637b217f4e808d465f8d84580a7b',\n",
       " 'ab145d9e02c24966b07f4321ce503c7c',\n",
       " 'b2d056bc0a9e4e89870641f2a2981b40',\n",
       " 'f0a0e3dc54984133b18cd6d1da34aed1',\n",
       " '1720a8a764a446d7a408f39fdf5f03f6',\n",
       " '3cdbcbbbb1e7402aac450985cdc2b898',\n",
       " '2debf4d04a5c4ef496e89d958ec288f6',\n",
       " '398b4032266e472cb0e1064597c6b2f8',\n",
       " '0567ce32422c4ceaaf9c6708b0e467e4',\n",
       " '068bf1a7819b4c2b840e12cf47a8e7a5',\n",
       " '2e1b13e869be461cb79ecccb170cd868',\n",
       " 'a2d953fa3f2d4aea988826cb3c0a62e2',\n",
       " '222098499d5f4f18a62ab308184a52c6',\n",
       " 'd55abff921cb4122b089c36c07e4d036',\n",
       " 'bd10f87389c844f38cf0a4e988d137d3',\n",
       " 'eaa7e1f6e5ed439db96f7db32a656ab3',\n",
       " '2cd933cca52746e485f913fdb4518616',\n",
       " 'b6124a98599144228cfd1671e8e0bb6f',\n",
       " 'a8aa8c607e1e42cb9b4daf67fcafc330',\n",
       " '9623e3824a3047749362cba3ac352609',\n",
       " '09a43508a1994f289a01fd947c05396e',\n",
       " '389b8971bf8d44a887a0150ea5c7d81c',\n",
       " 'f1db6b1d1d6f4ba5b43f1fcd2d2b9989',\n",
       " '6ea45e267cac4188aa8fa987f2a5f819',\n",
       " 'cd27d1ba400245deab86aecb2d609d5a',\n",
       " '9881fa5db30a48dd8b95dbeba3b54443',\n",
       " 'e0bef289d9eb4bbfae3f6f1fb26dc16b',\n",
       " '28281ff3708e4782a159c0399fffd546',\n",
       " 'b08184a166bf48e1994a8a38870a6c21',\n",
       " '5379ec228ab84913bdb6968a6fc34699',\n",
       " '443e9f478de346c1a23a2dbe1f51f977',\n",
       " 'c4d9d0c5861a4c75a27ca2a27af3b5f0',\n",
       " 'ac55f4dd8af5474eba68e0b2aa6b7f9c',\n",
       " '9fe6f02e9b78493f8bb5ea6b0f6a3fa5',\n",
       " 'fe32e3e2add0446facf7d403d3696cdd',\n",
       " '35d46b1065fc4005b58cc9abdd68110f',\n",
       " '2f2e4a127a564ec7b8c624ba9303e890',\n",
       " '45f29dc2487d42d68a30f4e33673e290',\n",
       " 'c054429c3e9340d1ab0591bc9d11b539',\n",
       " '5f9b3ebbffa4416ba3df4cdb12d1ac27',\n",
       " '9c387ce6413946728146ffce8a7a24a3',\n",
       " 'f86990c8ea894448910efb812b079241',\n",
       " '6e78528313714954b49cb766895e6707',\n",
       " 'fad2428abb0148e0882c220af6205639',\n",
       " '2153b971f56a4680b47e6ba21ae8da01',\n",
       " '6ede13cb1b944f7a8a549dd3a27734fb',\n",
       " '57c467e9014b4094a54257b3f993c0f8',\n",
       " '7b8eeb74d79443ffaf0877cf35c44a2c',\n",
       " '93720c6dfb7d4a08916dc374863b23f0',\n",
       " '9ad7088c41c94c14b148b00b9a91570c',\n",
       " 'b518e2675ff6440eb1101514962b1744',\n",
       " 'a01a118c3c9a4a6b8496e9f54a5f36eb',\n",
       " '82d811d4d8d640ca8e80d6c7977a9c7e',\n",
       " '99a2bff835a6490bbf48725225a4993f',\n",
       " '018d901d14ce48e9b9412f5ff2aaf725',\n",
       " '04e70ada8e6b4b80b6d93f7947473ab9',\n",
       " '5234af960eb449f88f4c29faac797324',\n",
       " 'ca2183a740b543a9a70583dc8dcf1e17',\n",
       " '3787009fd0974f029011cafce7d612ce',\n",
       " '33cfd80067e64d9596bca27fc53f9f4d',\n",
       " '2be6367d6e3443d4b99b0b49827b804c',\n",
       " '79d4602a7efd462ca765cd969352b005',\n",
       " '44425cb967bf49b7ac240e821324b87c',\n",
       " '7cdaf97a2aab4ec081d6e25387674464',\n",
       " '698e514f7b2e42a4a536d594afef955c',\n",
       " '7555d576f95d4adfb5852be0376b1a3d',\n",
       " 'be4794ce98c648299cf540ba4e82962c',\n",
       " '6371c53127234db781af518e554306a3',\n",
       " '8cc8988cb79544da83e7bf05f28d2c76',\n",
       " '7e8c212f35004d39b83e09b0c1e00003',\n",
       " '94dabe2b882b46b28c6dc1f1b0c229e4',\n",
       " '6a63efdcaf624524a2fef759a0325b9f',\n",
       " 'e340f7064a574efe8c0d0e949b7f0ebd',\n",
       " '4a95699ec0184fbc81f6e1cc816b1375']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qdrant_vectorstore.add_documents(all_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountResult(count=81)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qdrant_vectorstore._client.count(collection_name=\"my_documents\",)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll use the LCEL Runnable protocol to define the chain, allowing us to\n",
    "\n",
    "* pipe together components and functions in a transparent way\n",
    "* automatically trace our chain in `LangSmith`\n",
    "* get streaming, async, and batched calling out of the box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from qdrant_client.http.models import models\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=LLM_NAME)\n",
    "\n",
    "\n",
    "def retriever_with_score(input_dict):\n",
    "    list_results = qdrant_vectorstore.similarity_search_with_score(\n",
    "        input_dict[\"query\"],\n",
    "        k=3,\n",
    "        filter=models.Filter(\n",
    "            must=[\n",
    "                models.FieldCondition(\n",
    "                    key=\"metadata.user\",\n",
    "                    match=models.MatchText(text=\"manuel\"),\n",
    "                ),\n",
    "                models.FieldCondition(\n",
    "                    key=\"metadata.collection\",\n",
    "                    match=models.MatchText(text=\"blog\"),\n",
    "                ),\n",
    "            ]\n",
    "        ),\n",
    "    )\n",
    "    scores = [score for _, score in list_results]\n",
    "    sources = [doc for doc, _ in list_results]\n",
    "\n",
    "    return {\"query\": input_dict[\"query\"],\n",
    "            \"sources\": sources,\n",
    "            \"scores\": scores,\n",
    "            }\n",
    "\n",
    "\n",
    "def format_docs(input_dict):\n",
    "    input_dict[\"context\"] = \"\\n\\n\".join(\n",
    "        doc.page_content for doc in input_dict[\"sources\"])\n",
    "    return input_dict\n",
    "\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Keep the answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{query}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = RunnableLambda(retriever_with_score).assign(\n",
    "    answer=(format_docs | prompt | llm | StrOutputParser()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'Tell me about the approaches to Task Decomposition?',\n",
       " 'sources': [Document(metadata={'id': 'fake_id', 'user': 'manuel', 'collection': 'blog', '_id': '29b49a2ae87c4bdb87750d1d45578e75', '_collection_name': 'my_documents'}, page_content='. Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs. Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains. Self-Reflection#'),\n",
       "  Document(metadata={'id': 'fake_id', 'user': 'manuel', 'collection': 'blog', '_id': 'd4f94e3168cb4558b1e983b93252da18', '_collection_name': 'my_documents'}, page_content='1. Overview of a LLM-powered autonomous agent system. Component One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead. Task Decomposition#'),\n",
       "  Document(metadata={'id': 'fake_id', 'user': 'manuel', 'collection': 'blog', '_id': 'f1db6b1d1d6f4ba5b43f1fcd2d2b9989', '_collection_name': 'my_documents'}, page_content='The system comprises of 4 stages:\\n(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning. Instruction:')],\n",
       " 'scores': [0.8333333333333333, 0.5, 0.3333333333333333],\n",
       " 'answer': 'Task decomposition can be done in several ways:\\n\\n1. **Simple Prompting**: Using prompts like \"Steps for XYZ.\\\\n1.\" or \"What are the subgoals for achieving XYZ?\" to guide the LLM in breaking down tasks.\\n2. **Task-Specific Instructions**: Providing specific instructions tailored to the task, such as \"Write a story outline.\" for writing.\\n3. **Human Inputs**: Involving human guidance to assist in the decomposition process.\\n4. **LLM+P Approach**: Utilizing an external classical planner with the Planning Domain Definition Language (PDDL). This involves translating the problem into PDDL, generating a plan with a classical planner, and then converting the plan back into natural language. \\n\\nThese methods enable effective planning and management of complex tasks.'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = rag_chain.invoke({\"query\":\"Tell me about the approaches to Task Decomposition?\"})\n",
    "# retrieved_docs = rag_chain.invoke(\"Locality-Sensitive Hashing\")\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Task decomposition can be done in several ways:\n",
       "\n",
       "1. **Simple Prompting**: Using prompts like \"Steps for XYZ.\\n1.\" or \"What are the subgoals for achieving XYZ?\" to guide the LLM in breaking down tasks.\n",
       "2. **Task-Specific Instructions**: Providing specific instructions tailored to the task, such as \"Write a story outline.\" for writing.\n",
       "3. **Human Inputs**: Involving human guidance to assist in the decomposition process.\n",
       "4. **LLM+P Approach**: Utilizing an external classical planner with the Planning Domain Definition Language (PDDL). This involves translating the problem into PDDL, generating a plan with a classical planner, and then converting the plan back into natural language. \n",
       "\n",
       "These methods enable effective planning and management of complex tasks."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(output[\"answer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310_langchain",
   "language": "python",
   "name": ".venv_langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
